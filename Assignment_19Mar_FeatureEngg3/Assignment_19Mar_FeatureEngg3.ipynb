{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans.\n",
    "Min-Max scaling is a popular data preprocessing technique that is used to rescale the values of a numerical feature to a fixed range. The goal of this technique is to transform the data so that it can be more easily compared to other data or used as input to machine learning models that require features with similar scales.\n",
    "\n",
    "For example, let's say we have a dataset of ages that range from 20 to 80 years old. We want to scale these values so that they fall within a range of 0 to 1.\n",
    "\n",
    "This technique can be useful in many applications, including image processing, financial analysis, and machine learning."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans.\n",
    "The Unit Vector technique, also known as normalization, is a feature scaling technique that rescales the values of a numerical feature so that they have a length or magnitude of 1. This is achieved by dividing each value in the feature by the Euclidean norm of the feature vector.\n",
    "\n",
    "One key difference between Unit Vector scaling and Min-Max scaling is that Unit Vector scaling does not have a fixed range. Instead, the values are rescaled to have a magnitude of 1, which can be useful in applications where the direction of the vector is more important than its magnitude.\n",
    "\n",
    "This technique can be useful in applications such as natural language processing, where the direction of word vectors can convey important semantic information."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans.\n",
    "PCA, or Principal Component Analysis, is a widely used technique in data science and machine learning that can be used for dimensionality reduction. It works by transforming a dataset of possibly correlated variables into a new set of uncorrelated variables called principal components.\n",
    "\n",
    "The goal of PCA is to find the directions in the original feature space that have the most variation and to project the data onto those directions, thus reducing the dimensionality of the dataset while preserving the most important information.\n",
    "\n",
    "To perform PCA, we first calculate the covariance matrix of the original dataset. This matrix describes the pairwise relationships between the features in the dataset. We then calculate the eigenvectors and eigenvalues of this matrix. The eigenvectors represent the principal components, and the corresponding eigenvalues represent the amount of variance in the data that is explained by each principal component.\n",
    "\n",
    "We can then use the eigenvectors as a basis for a new coordinate system, and project the original dataset onto this new coordinate system to obtain the transformed dataset with reduced dimensionality.\n",
    "\n",
    "Here is an example of how to perform PCA using the scikit-learn library in Python:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: (100, 5)\n",
      "Transformed dataset shape: (100, 2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Generate a random dataset with 5 features\n",
    "X = np.random.rand(100, 5)\n",
    "\n",
    "# Instantiate a PCA object with 2 principal components\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit the PCA model to the dataset and transform the data\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Print the shape of the original and transformed datasets\n",
    "print('Original dataset shape:', X.shape)\n",
    "print('Transformed dataset shape:', X_pca.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA can be useful in many applications, including data visualization, feature extraction, and machine learning. By reducing the dimensionality of the data, it can help to improve the performance of machine learning algorithms, reduce overfitting, and make it easier to visualize and interpret complex datasets."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans.\n",
    "Feature extraction can be used to reduce the dimensionality of the data and to extract features that are more relevant for the task at hand. PCA can be used for feature extraction by identifying the most important directions in the feature space and projecting the data onto those directions.\n",
    "\n",
    "PCA can be useful in many applications where high-dimensional data needs to be processed, such as image recognition or natural language processing. By reducing the dimensionality of the data using PCA, we can improve the performance of machine learning algorithms and make it easier to visualize and interpret complex datasets."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans.\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset into a pandas dataframe\n",
    "df = pd.read_csv('food_delivery.csv')\n",
    "\n",
    "# Select the numerical features to be scaled\n",
    "num_features = ['price', 'rating', 'delivery_time']\n",
    "\n",
    "# Instantiate a MinMaxScaler object\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Scale the selected features in the dataset\n",
    "df[num_features] = scaler.fit_transform(df[num_features])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the dataset into a numpy array\n",
    "data = np.loadtxt('stock_prices.csv', delimiter=',')\n",
    "\n",
    "# Instantiate a PCA object with 5 principal components\n",
    "pca = PCA(n_components=5)\n",
    "\n",
    "# Fit the PCA model to the dataset and transform the data\n",
    "data_pca = pca.fit_transform(data)\n",
    "\n",
    "# or use the alternate way\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the dataset into a numpy array\n",
    "data = np.loadtxt('stock_prices.csv', delimiter=',')\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "data_std = scaler.fit_transform(data)\n",
    "\n",
    "# Compute the covariance matrix\n",
    "cov_matrix = np.cov(data_std.T)\n",
    "\n",
    "# Compute the eigenvectors and eigenvalues\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "\n",
    "# Sort the eigenvectors by their corresponding eigenvalues\n",
    "idx = eigenvalues.argsort()[::-1]\n",
    "eigenvectors = eigenvectors[:,idx]\n",
    "eigenvalues = eigenvalues[idx]\n",
    "\n",
    "# Select the top k eigenvectors\n",
    "k = 5\n",
    "top_eigenvectors = eigenvectors[:,:k]\n",
    "\n",
    "# Project the data onto the selected principal components\n",
    "data_pca = np.dot(data_std, top_eigenvectors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.        ],\n",
       "       [-0.57894737],\n",
       "       [-0.05263158],\n",
       "       [ 0.47368421],\n",
       "       [ 1.        ]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ans.\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "a = pd.DataFrame([1,5,10,15,20])\n",
    "min_max = MinMaxScaler(feature_range=(-1,1))\n",
    "min_max.fit_transform(a)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans.\n",
    "To perform feature extraction using PCA, we first need to standardize the data.\n",
    "The number of principal components to retain depends on the amount of variance we want to explain in the original dataset. One common approach is to choose the minimum number of principal components that explain a certain percentage of the total variance, such as 95% or 99%.\n",
    "let's assume we want to explain at least 95% of the variance in the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of principal components to retain: 2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create a dataset with random values\n",
    "data = [[170, 70, 25, 'M', 120], \n",
    "        [165, 65, 30, 'F', 130], \n",
    "        [180, 75, 40, 'M', 140], \n",
    "        [175, 80, 35, 'F', 130], \n",
    "        [172, 72, 28, 'M', 120]]\n",
    "\n",
    "# Extract the numerical features and standardize them\n",
    "numerical_data = [[row[0], row[1], row[2], row[4]] for row in data]\n",
    "scaler = StandardScaler()\n",
    "standardized_data = scaler.fit_transform(numerical_data)\n",
    "\n",
    "# Compute the principal components and explained variance\n",
    "pca = PCA()\n",
    "pca.fit(standardized_data)\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "# Choose the number of principal components to retain\n",
    "total_variance = 0\n",
    "num_components = 0\n",
    "for variance in explained_variance:\n",
    "    total_variance += variance\n",
    "    num_components += 1\n",
    "    if total_variance >= 0.95:\n",
    "        break\n",
    "\n",
    "print(f\"Number of principal components to retain: {num_components}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
