{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58b03c33",
   "metadata": {},
   "source": [
    "### Q1. What is the relationship between polynomial functions and kernel functions in machine learning algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366690d6",
   "metadata": {},
   "source": [
    "Polynomial functions and kernel functions are both used in machine learning algorithms for feature transformation, which can help make the data more separable and improve classification accuracy.\n",
    "\n",
    "Polynomial functions are a type of feature transformation that can be used to transform input features into higher-dimensional space. For example, given a 2-dimensional input feature vector x = [x1, x2], we can apply a polynomial transformation of degree 2 to obtain a 6-dimensional feature vector z = [1, x1, x2, x1^2, x1x2, x2^2]. This transformation can be used to fit a linear model in the transformed feature space, which can lead to better classification accuracy.\n",
    "\n",
    "Kernel functions, on the other hand, are a more general type of feature transformation that can be used to transform input features into a potentially infinite-dimensional space. Kernel functions are used in kernel methods, which are a class of algorithms that operate in a feature space induced by a kernel function. Examples of kernel functions include the polynomial kernel, which computes the dot product of two polynomial feature vectors, and the radial basis function (RBF) kernel, which maps each data point to an infinite-dimensional feature space using a Gaussian function.\n",
    "\n",
    "In essence, the polynomial kernel is a specific type of kernel function that applies a polynomial transformation to the input features before computing the dot product in the feature space, whereas polynomial functions are a specific type of feature transformation that can be used to transform input features into a higher-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a208a7e1",
   "metadata": {},
   "source": [
    "### Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0caec3c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "#To implement an SVM with a polynomial kernel in Python using Scikit-learn, you can follow these steps:\n",
    "\n",
    "#Step 1: Import the necessary libraries\n",
    "from sklearn import svm\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Step 2: Generate a sample dataset for classification using make_classification() method\n",
    "X, y = make_classification(n_samples=100, n_features=4, n_classes=2, random_state=42)\n",
    "\n",
    "#Step 3: Split the dataset into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "#Step 4: Create an SVM classifier with polynomial kernel using svm.SVC class with kernel='poly' argument and other hyperparameters like C, degree, coef0, and gamma set according to your dataset.\n",
    "clf = svm.SVC(kernel='poly', degree=3, coef0=1, C=1, gamma='scale')\n",
    "\n",
    "#Here, degree is the degree of the polynomial kernel, coef0 is the independent term in the kernel function, C is the regularization parameter, and gamma is the kernel coefficient.\n",
    "\n",
    "#Step 5: Train the classifier on the training dataset\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "#Step 6: Make predictions on the testing dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "#Step 7: Evaluate the classifier using accuracy score\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy:', accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8deb357e",
   "metadata": {},
   "source": [
    "### Q3. How does increasing the value of epsilon affect the number of support vectors in SVR? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8011b45a",
   "metadata": {},
   "source": [
    "In Support Vector Regression (SVR), the parameter epsilon (ε) determines the width of the ε-insensitive zone around the regression line. The ε-insensitive zone is a region where errors smaller than ε are ignored and not penalized in the optimization process.\n",
    "\n",
    "Increasing the value of epsilon will result in a larger ε-insensitive zone. This means that more training data points will be considered to have zero error and will not contribute to the formation of support vectors. As a result, the number of support vectors decreases as the value of epsilon increases.\n",
    "\n",
    "Caution:- A large value of epsilon can lead to underfitting, while a small value of epsilon can lead to overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d2b5dc",
   "metadata": {},
   "source": [
    "### Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works and provide examples of when you might want to increase or decrease its value? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6493ff48",
   "metadata": {},
   "source": [
    "Here is an overview of each parameter and how they can be tuned to optimize the performance of an SVR model:\n",
    "\n",
    "**Kernel function**: The choice of kernel function determines the shape of the decision boundary and can significantly affect the performance of the model. The most commonly used kernel functions are linear, polynomial, and radial basis function (RBF). A linear kernel is used for linearly separable data, while polynomial and RBF kernels are used for non-linear data. In general, the RBF kernel is the most commonly used kernel in SVR as it can model complex non-linear relationships between the input features and the target variable.\n",
    "\n",
    "**C parameter**: The C parameter determines the trade-off between model complexity and error. A small value of C will result in a wider margin and fewer support vectors, which can lead to underfitting. Conversely, a large value of C will result in a narrower margin and more support vectors, which can lead to overfitting. In general, it is recommended to use a cross-validation technique to find the optimal value of C that balances the trade-off between model complexity and error.\n",
    "\n",
    "**Epsilon parameter**: The epsilon parameter determines the width of the ε-insensitive zone around the regression line. A larger value of epsilon will result in a larger ε-insensitive zone, which can result in fewer support vectors and a simpler model. Conversely, a smaller value of epsilon will result in a smaller ε-insensitive zone, which can result in more support vectors and a more complex model.\n",
    "\n",
    "**Gamma parameter**: The gamma parameter determines the shape of the decision boundary in RBF kernels. A small value of gamma will result in a wider curve and a smoother decision boundary, which can lead to underfitting. Conversely, a large value of gamma will result in a narrower curve and a more complex decision boundary, which can lead to overfitting.\n",
    "\n",
    "Here are some examples of when you might want to increase or decrease each parameter:\n",
    "\n",
    "`Kernel function`: Use a linear kernel for linearly separable data and RBF kernel for non-linear data.\n",
    "\n",
    "`C parameter`: Increase the value of C if the training error is high or the model is underfitting. Decrease the value of C if the model is overfitting.\n",
    "\n",
    "`Epsilon parameter`: Increase the value of epsilon if the model is overfitting and the number of support vectors is too high. Decrease the value of epsilon if the model is underfitting.\n",
    "\n",
    "`Gamma parameter`: Increase the value of gamma if the model is underfitting and the decision boundary is too smooth. Decrease the value of gamma if the model is overfitting and the decision boundary is too complex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036c0a77",
   "metadata": {},
   "source": [
    "### Q5. Assignment:\n",
    "1. Import the necessary libraries and load the dataset\n",
    "2. Split the dataset into training and testing sets\n",
    "3. Preprocess the data using any technique of your choice (e.g. scaling, normalization)\n",
    "4. Create an instance of the SVC classifier and train it on the training data\n",
    "5. Use the trained classifier to predict the labels of the testing data\n",
    "6. Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy,precision, recall, F1-score)\n",
    "7. Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomisedSearchCV to improve its performance\n",
    "8. Train the tuned classifier on the entire dataset\n",
    "9. Save the trained classifier to a file for future use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "085096cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9666666666666667\n",
      "Precision: 0.9722222222222222\n",
      "Recall: 0.9629629629629629\n",
      "F1-score: 0.9658994032395567\n",
      "Best hyperparameters: {'C': 10, 'degree': 2, 'kernel': 'linear'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['svm_clf_tuned.pkl']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "#Joblib is a Python library for running computationally intensive tasks in parallel. It provides a set of functions for performing operations in parallel on large data sets and for caching the results of computationally expensive functions.\n",
    "\n",
    "# Load the dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess the data using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create an instance of the SVC classifier and train it on the training data\n",
    "svm_clf=SVC(kernel=\"linear\",C=1)\n",
    "svm_clf.fit(X_train,y_train)\n",
    "\n",
    "# Use the trained classifier to predict the labels of the testing data\n",
    "y_pred = svm_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the classifier using accuracy, precision, recall, and F1-score\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('Precision:', precision_score(y_test, y_pred, average='macro'))\n",
    "print('Recall:', recall_score(y_test, y_pred, average='macro'))\n",
    "print('F1-score:', f1_score(y_test, y_pred, average='macro'))\n",
    "#The macro average is the arithmetic mean of the individual class related to precision, memory, and f1 score\n",
    "\n",
    "# Tune the hyperparameters of the SVC classifier using GridSearchCV\n",
    "params = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf', 'poly'], 'degree': [2, 3, 4]}\n",
    "svm_grid = GridSearchCV(SVC(), params, cv=5)\n",
    "svm_grid.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters found by GridSearchCV\n",
    "print('Best hyperparameters:', svm_grid.best_params_)\n",
    "\n",
    "# Train the tuned classifier on the entire dataset\n",
    "svm_clf_tuned = svm_grid.best_estimator_\n",
    "svm_clf_tuned.fit(X, y)\n",
    "\n",
    "# Save the trained classifier to a file for future use\n",
    "joblib.dump(svm_clf_tuned, 'svm_clf_tuned.pkl')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
