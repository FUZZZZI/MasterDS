{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some algorithms that are not affected by missing values."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans.\n",
    "Missing values in a dataset refer to the absence of a value for a particular observation or attribute in the dataset. \n",
    "\n",
    "It is essential to handle missing values because they can affect the accuracy and validity of the analysis and modeling results. Missing values can lead to biased estimates of parameters, reduce the power of statistical tests, and distort the relationship between variables.\n",
    "\n",
    "Some algorithms that are not affected by missing values include:\n",
    "Decision trees: Decision trees can handle missing values by choosing the best split based on the available data for each attribute.\n",
    "\n",
    "Random Forests: Random forests can handle missing values by imputing the missing values using the mean or median of the available values in the same column.\n",
    "\n",
    "K-Nearest Neighbors (KNN): KNN can handle missing values by ignoring the missing values during the calculation of the distance between instances.\n",
    "\n",
    "Support Vector Machines (SVM): SVM can handle missing values by imputing the missing values with the mean or median of the available values in the same column.\n",
    "\n",
    "Naive Bayes: Naive Bayes can handle missing values by ignoring the missing values during the estimation of the probabilities."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: List down techniques used to handle missing data.  Give an example of each with python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B   C\n",
      "0  1.0  5.0   9\n",
      "3  4.0  8.0  12\n",
      "     A    B     C\n",
      "0  1.0  5.0   9.0\n",
      "1  4.0  8.0  12.0\n",
      "     A    B     C\n",
      "0  1.0  5.0   9.0\n",
      "1  4.0  8.0  12.0\n"
     ]
    }
   ],
   "source": [
    "# Ans.\n",
    "# 1. Deletion techniques: In this approach, the rows or columns with missing values are deleted from the dataset.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Creating a sample dataset\n",
    "data = pd.DataFrame({'A': [1, 2, np.nan, 4], 'B': [5, np.nan, np.nan, 8], 'C': [9, 10, 11, 12]})\n",
    "\n",
    "# Dropping rows with missing values\n",
    "data.dropna(inplace=True)\n",
    "print(data)\n",
    "\n",
    "# 2. Imputation techniques: In this approach, the missing values are replaced with estimated values based on the available data.\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Imputing missing values with the mean\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "data_imputed = pd.DataFrame(imputer.fit_transform(data), columns=data.columns)\n",
    "print(data_imputed)\n",
    "\n",
    "# 3. Regression techniques: In this approach, missing values are imputed using a regression model based on the available data.\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# Imputing missing values with a regression model\n",
    "imputer = IterativeImputer()\n",
    "data_imputed = pd.DataFrame(imputer.fit_transform(data), columns=data.columns)\n",
    "print(data_imputed)\n",
    "\n",
    "# 4. Mean substitution techniques: In this approach, the missing values are replaced with the mean value of the available data.\n",
    "\n",
    "# Fill missing values with the mean of each column\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "\n",
    "# 5. Forward or Backward Fill: Replacing missing data with the last known value or the next known value.\n",
    "\n",
    "# Forward fill missing values\n",
    "df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Backward fill missing values\n",
    "df.fillna(method='bfill', inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans.\n",
    "Imbalanced data refers to a situation where the number of observations in each class of a binary or multi-class classification problem is not evenly distributed.\n",
    "If imbalanced data is not handled, the predictive model that is built on such data may end up being biased towards the majority class. \n",
    "As a result, the model may not perform well on the minority class, leading to poor predictive performance in real-world scenarios."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and downsampling are required."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans.\n",
    "Up-sampling is the process of increasing the number of samples in the minority class to match the number of samples in the majority class. This can be done by duplicating existing samples, generating synthetic samples using techniques such as SMOTE (Synthetic Minority Over-sampling Technique), or a combination of both. Up-sampling can be useful when the minority class is underrepresented, and we need to increase the model's exposure to the minority class.\n",
    "\n",
    "Down-sampling is the process of reducing the number of samples in the majority class to match the number of samples in the minority class.\n",
    "\n",
    "Ex: suppose we have a dataset with 1000 samples, of which 900 belong to the majority class and 100 belong to the minority class.\n",
    "we can use up-sampling to generate synthetic samples for the minority class and increase the number of minority class samples to match the majority class samples, say 900 or\n",
    "we can use down-sampling to randomly remove samples from the majority class until the number of samples in the majority class matches the number of samples in the minority class, say 100. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5: What is data Augmentation? Explain SMOTE."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans.\n",
    "Data augmentation is a technique used to increase the size of a dataset by creating new samples from the existing ones. \n",
    "One popular data augmentation technique is SMOTE (Synthetic Minority Over-sampling Technique). SMOTE is used to increase the number of samples in the minority class by creating synthetic samples that are similar to the existing minority samples.\n",
    "\n",
    "Steps:-\n",
    "1. For each minority sample, k nearest neighbors are identified in the minority class.\n",
    "2. A new synthetic sample is generated by randomly selecting one of the k neighbors and adding a scaled difference between the feature values of the selected neighbor and the original sample.\n",
    "3. The process is repeated until the desired number of synthetic samples is generated."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6: What are outliers in a dataset? Why is it essential to handle outliers?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans.\n",
    "Outliers are observations or data points that are significantly different from other observations in a dataset. These observations can be exceptionally high or low in comparison to the rest of the data points. Outliers can occur due to various reasons, such as measurement errors, experimental errors, data entry errors, or rare events.\n",
    "\n",
    "It is essential to handle outliers because they can have a significant impact on the statistical properties of the dataset and the model's performance. Outliers can affect the mean, standard deviation, and correlation coefficients, leading to biased and inaccurate results. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7: You are working on a project that requires analyzing customer data. However, you notice that some of the data is missing. What are some techniques you can use to handle the missing data in your analysis?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans.\n",
    "Handling missing data is an essential step in data analysis as missing data can significantly affect the accuracy and reliability of the analysis results. \n",
    "\n",
    "Techniques:-\n",
    "1. Deletion\n",
    "2. Imputation\n",
    "3. Forwad or backward fill\n",
    "4. Regression\n",
    "5. Substitution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are some strategies you can use to determine if the missing data is missing at random or if there is a pattern to the missing data?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans.\n",
    "1. Visual inspection: One way to detect the pattern of missing data is to plot the data and look for any noticeable patterns. If there is any observable pattern, it could indicate that the missing values are not missing at random.\n",
    "\n",
    "2. Statistical tests: One such test is the Little's MCAR (Missing Completely At Random) test. This test is based on a chi-square statistic and tests the hypothesis that the missing data are completely at random. If the p-value is greater than 0.05, we can conclude that the data are missing at random.\n",
    "\n",
    "3. Domain knowledge: Sometimes, domain knowledge can be helpful in identifying patterns in missing data. For example, if you are studying patient data and find that data is missing for certain age groups, it may indicate that the missing values are not random and that there is a pattern to the missing data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the dataset do not have the condition of interest, while a small percentage do. What are some strategies you can use to evaluate the performance of your machine learning model on this imbalanced dataset?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans.\n",
    "1. Confusion Matrix: Confusion matrix is a table that summarizes the performance of a classification model by comparing predicted and actual class labels. It includes true positive (TP), true negative (TN), false positive (FP), and false negative (FN) values. Confusion matrix can help to calculate evaluation metrics such as precision, recall, and F1 score.\n",
    "\n",
    "Precision, Recall, and F1 Score: Precision measures the proportion of positive predictions that are correct, whereas recall measures the proportion of actual positive cases that are correctly predicted. F1 score is the harmonic mean of precision and recall. These metrics are better suited for evaluating the performance of a model on imbalanced datasets.\n",
    "\n",
    "2. Receiver Operating Characteristic (ROC) Curve: ROC curve is a plot of true positive rate (TPR) against the false positive rate (FPR) at different threshold values. It helps to visualize the performance of a classification model and can be used to calculate the area under the curve (AUC) score.\n",
    "\n",
    "3. Stratified Sampling: Stratified sampling is a method to ensure that the training and testing datasets have a balanced distribution of classes. This can help to improve the performance of a model on an imbalanced dataset.\n",
    "\n",
    "4. Resampling Techniques: Resampling techniques such as oversampling and undersampling can be used to balance the dataset. Oversampling involves replicating the minority class samples, while undersampling involves removing some samples from the majority class. However, these techniques should be used with caution as they can introduce bias into the model.\n",
    "\n",
    "5. Cost-Sensitive Learning: Cost-sensitive learning is a technique that assigns different misclassification costs to different classes. This can help to improve the performance of a model on an imbalanced dataset by penalizing more for misclassifying the minority class."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to balance the dataset and down-sample the majority class?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans.\n",
    "1. Undersampling: \n",
    "2 types: Random undersampling involves randomly selecting a subset of samples from the majority class, while systematic undersampling involves selecting samples at a fixed interval.\n",
    "2. Stratified sampling: Stratified sampling is a technique where samples are selected from each class in proportion to their representation in the dataset. \n",
    "3. Ensemble methods: Ensemble methods such as bagging, boosting, and stacking can be used to balance the dataset and improve the performance of the model. These methods combine multiple models to make predictions and can be particularly effective when dealing with imbalanced datasets.\n",
    "4. Cost-sensitive learning: Cost-sensitive learning is a technique where misclassification costs are assigned to each class based on their importance. This can help to improve the performance of the model by penalizing more for misclassifying the minority class."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a project that requires you to estimate the occurrence of a rare event. What methods can you employ to balance the dataset and up-sample the minority class?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans.\n",
    "1. Oversampling\n",
    "2. Synthetic Minority Over-sampling Technique (SMOTE): SMOTE is a technique that generates synthetic samples by interpolating between existing minority class samples. This can help to balance the dataset and improve the performance of the model.\n",
    "3. Adaptive Synthetic Sampling (ADASYN): ADASYN is a variation of SMOTE that generates synthetic samples with a higher density in regions where the minority class is sparsely represented.\n",
    "4. Ensemble methods\n",
    "5. Cost-sensitive learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
