{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8196c14c",
   "metadata": {},
   "source": [
    "###  Q1. What is Gradient Boosting Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e040ba9e",
   "metadata": {},
   "source": [
    "Gradient Boosting Regression (GBR) is a popular ensemble learning technique used for regression problems. It is an extension of the gradient descent algorithm, where instead of minimizing a cost function for a single model, a sequence of models are trained to minimize the overall error. The main idea behind GBR is to fit a sequence of weak learners, such as decision trees, to the residuals of the previous models, where the residuals are the differences between the predicted and actual values.\n",
    "\n",
    "Here's how the GBR algorithm works:\n",
    "\n",
    "1. Initialize the model with a constant value, such as the mean of the target variable.\n",
    "\n",
    "2. For each iteration, fit a weak learner, such as a decision tree, to the negative gradient of the loss function with respect to the current model's predictions. The negative gradient represents the direction of steepest descent, or the direction in which the loss function decreases the most.\n",
    "\n",
    "3. Use the weak learner to predict the residuals, which are the differences between the actual and predicted values of the target variable.\n",
    "\n",
    "4. Update the model by adding the predictions of the weak learner, weighted by a learning rate, to the previous model's predictions. The learning rate controls the contribution of each weak learner to the final model and helps to prevent overfitting.\n",
    "\n",
    "5. Repeat steps 2 to 4 until the desired number of iterations or until the residuals converge to zero.\n",
    "\n",
    "6. Output the final model, which is the sum of all the weak learners weighted by their respective learning rates.\n",
    "\n",
    "GBR is a powerful algorithm that can achieve high accuracy on regression problems, especially when combined with other techniques such as cross-validation and regularization. However, it can also be prone to overfitting and requires careful tuning of hyperparameters such as the learning rate, the number of iterations, and the depth of the decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77c8c1c",
   "metadata": {},
   "source": [
    "###  Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7cba900",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.estimators = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Initialize the model with the mean of the target variable\n",
    "        self.mean = np.mean(y)\n",
    "        self.estimators = [self.mean] * self.n_estimators\n",
    "        \n",
    "        # Loop over the number of estimators and fit a weak learner to the residuals\n",
    "        for i in range(self.n_estimators):\n",
    "            # Calculate the negative gradient of the loss function with respect to the current model's predictions\n",
    "            residuals = y - self.predict(X)\n",
    "            negative_gradient = -residuals\n",
    "            \n",
    "            # Fit a decision tree to the negative gradient\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, negative_gradient)\n",
    "            \n",
    "            # Update the model by adding the predictions of the weak learner, weighted by the learning rate\n",
    "            self.estimators[i] += self.learning_rate * tree.predict(X)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        # Make predictions by summing the predictions of all the weak learners\n",
    "        y_pred = np.zeros(len(X))\n",
    "        for i in range(self.n_estimators):\n",
    "            y_pred += self.learning_rate * self.estimators[i]\n",
    "        return y_pred + self.mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53b4ae86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 35629.77\n",
      "R-squared: -28.94\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate a random dataset\n",
    "X, y = make_regression(n_samples=50, n_features=1, noise=10)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)\n",
    "\n",
    "# Train the model\n",
    "gb = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = gb.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance using mean squared error and R-squared\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error: {:.2f}\".format(mse))\n",
    "print(\"R-squared: {:.2f}\".format(r2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04c4d02",
   "metadata": {},
   "source": [
    "###  Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "398fddc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters:  {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 200}\n",
      "Best Mean Squared Error: 198.71\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Define the hyperparameters to search over\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'learning_rate': [0.01, 0.1, 0.5],\n",
    "    'max_depth': [3, 5, 7],\n",
    "}\n",
    "\n",
    "# Initialize the GradientBoostingRegressor\n",
    "gb = GradientBoostingRegressor()\n",
    "\n",
    "# Perform grid search to find the best hyperparameters\n",
    "search = GridSearchCV(gb, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters and the corresponding mean squared error\n",
    "print(\"Best Hyperparameters: \", search.best_params_)\n",
    "print(\"Best Mean Squared Error: {:.2f}\".format(-search.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd49c57",
   "metadata": {},
   "source": [
    "###  Q4. What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5c0c25",
   "metadata": {},
   "source": [
    "In Gradient Boosting, a weak learner is a decision tree model that performs slightly better than random guessing on the training set. The goal of the Gradient Boosting algorithm is to iteratively combine weak learners to create a strong learner. Each weak learner is trained on the residual errors of the previous weak learner. This approach allows the algorithm to focus on the patterns in the data that were not captured by the previous models.\n",
    "\n",
    "A decision tree is a popular choice for a weak learner in Gradient Boosting because it is easy to implement and can handle both categorical and continuous variables. However, other models such as linear regression or neural networks can also be used as weak learners in Gradient Boosting. The key characteristic of a weak learner is that it is simple and has a low variance, which allows it to be combined with other weak learners without overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde2ce90",
   "metadata": {},
   "source": [
    "###  Q5. What is the intuition behind the Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba98a9d",
   "metadata": {},
   "source": [
    "The intuition behind Gradient Boosting is to iteratively improve the predictions of a model by combining weak learners. At each iteration, the algorithm trains a new weak learner to predict the residual errors of the previous model. The idea is that by focusing on the patterns in the data that were not captured by the previous models, the algorithm can gradually improve the overall accuracy of the model.\n",
    "\n",
    "The algorithm starts with a simple model, such as a decision tree, and trains it on the training data. It then calculates the residual errors between the predictions of the model and the true labels of the training data. The next model is trained on these residual errors, rather than the original labels, and the process is repeated until a pre-defined stopping criterion is met.\n",
    "\n",
    "The predictions of each model are then combined to create the final prediction. The weights of each model are determined by the errors of the previous models, so the algorithm places more emphasis on the areas of the data where the previous models performed poorly.\n",
    "\n",
    "The Gradient Boosting algorithm is particularly effective because it can handle both continuous and categorical data, and can capture non-linear relationships between the features and the target variable. It can also handle missing data and outliers, making it a robust choice for a wide range of problems. However, it can be computationally expensive and may require careful tuning of the hyperparameters to avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a7c4bd",
   "metadata": {},
   "source": [
    "###  Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefc4103",
   "metadata": {},
   "source": [
    "Gradient Boosting builds an ensemble of weak learners by iteratively training a series of simple models, typically decision trees, and combining their predictions to create a strong learner. The algorithm works in the following steps:\n",
    "\n",
    "**Initialize the ensemble**: The first step is to initialize the ensemble by selecting a simple model, such as a decision tree, and training it on the training data.\n",
    "\n",
    "**Calculate the residuals**: The next step is to calculate the residual errors between the predictions of the current model and the true labels of the training data.\n",
    "\n",
    "**Train a new weak learner**: A new weak learner is trained on the residual errors, rather than the original labels. The new model is trained to predict the residual errors left by the previous model.\n",
    "\n",
    "**Add the new weak learner to the ensemble**: The new model is added to the ensemble, and its predictions are combined with the predictions of the previous models. The weights of each model in the ensemble are determined by the errors of the previous models, so the algorithm places more emphasis on the areas of the data where the previous models performed poorly.\n",
    "\n",
    "**Repeat**: Steps 2-4 are repeated for a fixed number of iterations or until a pre-defined stopping criterion is met.\n",
    "\n",
    "**Final prediction**: The final prediction is a combination of the predictions of all the models in the ensemble.\n",
    "\n",
    "The key idea behind Gradient Boosting is to iteratively add simple models to the ensemble that improve the overall accuracy of the predictions. The algorithm focuses on the patterns in the data that were not captured by the previous models, allowing it to gradually improve the overall accuracy of the model. By combining the predictions of multiple weak learners, the final model is able to capture complex non-linear relationships between the features and the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156d36e7",
   "metadata": {},
   "source": [
    "###  Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caea2377",
   "metadata": {},
   "source": [
    "The mathematical intuition behind Gradient Boosting algorithm involves several steps:\n",
    "\n",
    "**Define the loss function**: The first step is to define a loss function that measures the difference between the predicted values and the true values. The most common loss function used in Gradient Boosting is the mean squared error (MSE), but other loss functions can also be used.\n",
    "\n",
    "**Initialize the model**: The next step is to initialize the model by selecting a simple model, such as a decision tree, and fitting it to the training data.\n",
    "\n",
    "**Calculate the negative gradient of the loss function**: The negative gradient of the loss function with respect to the predicted values is calculated for each observation in the training data. This represents the direction in which the predictions need to be adjusted to minimize the loss function.\n",
    "\n",
    "**Fit a new model to the negative gradient**: A new model is trained to predict the negative gradient of the loss function. This model is trained to correct the errors made by the previous model.\n",
    "\n",
    "**Combine the models**: The new model is combined with the previous model to form a more accurate model. This is done by adding the predictions of the new model to the predictions of the previous model, with a weight assigned to each model based on its performance.\n",
    "\n",
    "**Repeat**: Steps 3-5 are repeated for a fixed number of iterations or until a pre-defined stopping criterion is met.\n",
    "\n",
    "**Final prediction**: The final prediction is the sum of the predictions of all the models in the ensemble.\n",
    "\n",
    "The intuition behind Gradient Boosting is that it iteratively adds weak models to the ensemble that improve the overall accuracy of the predictions. By focusing on the patterns in the data that were not captured by the previous models, the algorithm is able to gradually improve the overall accuracy of the model. The use of negative gradients allows the algorithm to correct the errors made by the previous model and move towards the optimal solution. The weights assigned to each model in the ensemble ensure that the algorithm places more emphasis on the areas of the data where the previous models performed poorly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
