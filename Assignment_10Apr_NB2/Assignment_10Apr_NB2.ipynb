{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10c87c10",
   "metadata": {},
   "source": [
    "### Q1. A company conducted a survey of its employees and found that 70% of the employees use the company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the probability that an employee is a smoker given that he/she uses the health insurance plan? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cb06b4",
   "metadata": {},
   "source": [
    "Event A: An employee uses the company's health insurance plan.\n",
    "Event S: An employee is a smoker.\n",
    "\n",
    "We are given the following probabilities:\n",
    "\n",
    "P(A) = 0.70 (probability that an employee uses the health insurance plan)\n",
    "\n",
    "P(S|A) = **0.40** (probability that an employee is a smoker given that he/she uses the health insurance plan)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fba583",
   "metadata": {},
   "source": [
    "### Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45723f84",
   "metadata": {},
   "source": [
    "1. **Bernoulli Naive Bayes**: It is used for binary features, where each feature can take on only two values, typically 0 or 1. In other words, Bernoulli Naive Bayes assumes that each feature is a binary variable, indicating the presence or absence of a particular feature in a given instance. It is often used for tasks where the presence or absence of a particular feature is important, such as spam classification where the presence or absence of certain keywords may be indicative of spam or non-spam emails.\n",
    "\n",
    "2. **Multinomial Naive Bayes**: It is used for features that represent discrete counts or frequencies, such as word counts in text data. Multinomial Naive Bayes assumes that features follow a multinomial distribution, which is a generalization of the Bernoulli distribution. It is often used for tasks where the frequency or occurrence of different discrete features is important, such as document classification where the frequency of different words in a document may be indicative of its category, such as topic or sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb4a095",
   "metadata": {},
   "source": [
    "### Q3. How does Bernoulli Naive Bayes handle missing values? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbf4a82",
   "metadata": {},
   "source": [
    "There are several approaches that can be used to handle missing values in Bernoulli Naive Bayes:\n",
    "\n",
    "1. **Deletion**: Instances or samples with missing values can be deleted from the dataset. However, this approach may lead to loss of information and reduced sample size, and should be used with caution, especially if the missing values are not randomly distributed.\n",
    "\n",
    "2. **Imputation**: Missing values can be imputed or filled in with estimated values. For binary features, missing values can be imputed using various methods, such as replacing them with the mode (most frequent value) of that feature in the dataset, or using statistical techniques like expectation-maximization (EM) algorithm or k-nearest neighbors (KNN) imputation.\n",
    "\n",
    "3. **eparate category for missing values**: Another approach is to treat missing values as a separate category or a separate binary value, distinct from 0 and 1. This approach allows the missing values to be explicitly modeled as a separate category, which can be useful if the missingness is informative or if the presence of missing values carries meaningful information for the classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e85de2",
   "metadata": {},
   "source": [
    "### Q4. Can Gaussian Naive Bayes be used for multi-class classification? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd03a9ae",
   "metadata": {},
   "source": [
    "Yes, Gaussian Naive Bayes can be used for multi-class classification tasks. In Gaussian Naive Bayes, it is assumed that the continuous features of the data follow a Gaussian or normal distribution. Each class is modeled by its own Gaussian distribution, characterized by the mean and variance of the feature values for that class. The probability density function (PDF) of the Gaussian distribution is used to estimate the likelihood of observing a particular feature value given a class.\n",
    "\n",
    "For multi-class classification, Gaussian Naive Bayes can be extended to handle multiple classes by using a one-vs-rest or one-vs-one approach. In the one-vs-rest approach, a separate Gaussian distribution is fitted for each class, modeling the distribution of feature values for that class compared to the rest of the classes. The class with the highest likelihood is then predicted as the final class label. In the one-vs-one approach, a separate Gaussian distribution is fitted for each pair of classes, and a majority voting scheme is used to determine the final class label based on the most frequent class predicted by the individual binary classifiers.\n",
    "\n",
    "It is important to note that Gaussian Naive Bayes assumes that the continuous features of the data follow a Gaussian distribution, which may not always be the case in practice. Therefore, it is essential to assess the underlying assumptions of the data and ensure that the Gaussian assumption is reasonable before applying Gaussian Naive Bayes for multi-class classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1e9598",
   "metadata": {},
   "source": [
    "### Q5. Assignment:\n",
    "Data preparation:\n",
    "\n",
    "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message is spam or not based on several input features.\n",
    "\n",
    "Implementation:\n",
    "\n",
    "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the \n",
    "scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the \n",
    "dataset. You should use the default hyperparameters for each classifier.\n",
    "\n",
    "\n",
    "Results:\n",
    "\n",
    "Report the following performance metrics for each classifier:\n",
    "\n",
    "Accuracy\n",
    "\n",
    "Precision\n",
    "\n",
    "Recall\n",
    "\n",
    "F1 score\n",
    "\n",
    "\n",
    "Discussion:\n",
    "\n",
    "Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is the case? Are there any limitations of Naive Bayes that you observed?\n",
    "\n",
    "\n",
    "Conclusion:\n",
    "\n",
    "Summarise your findings and provide some suggestions for future work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e87f5894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli Naive Bayes:\n",
      "Accuracy: 0.887\n",
      "Precision: 0.885\n",
      "Recall: 0.813\n",
      "F1 Score: 0.847\n",
      "\n",
      "Multinomial Naive Bayes:\n",
      "Accuracy: 0.791\n",
      "Precision: 0.741\n",
      "Recall: 0.701\n",
      "F1 Score: 0.720\n",
      "\n",
      "Gaussian Naive Bayes:\n",
      "Accuracy: 0.811\n",
      "Precision: 0.684\n",
      "Recall: 0.949\n",
      "F1 Score: 0.794\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.naive_bayes import BernoulliNB,GaussianNB,MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "\n",
    "#Load the dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\"\n",
    "df=pd.read_csv(url, header=None)\n",
    "\n",
    "#Split the dataset\n",
    "X=df.iloc[:,:-1] #Features\n",
    "y=df.iloc[:,-1]  #Target\n",
    "\n",
    "# Instantiate Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers\n",
    "bernoulli_nb = BernoulliNB()\n",
    "multinomial_nb = MultinomialNB()\n",
    "gaussian_nb = GaussianNB()\n",
    "\n",
    "#train-test split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=42)\n",
    "\n",
    "# Perform 10-fold cross-validation and compute performance metrics for Bernoulli Naive Bayes\n",
    "bernoulli_nb_scores_acc = cross_val_score(bernoulli_nb, X_train, y_train, cv=10, scoring='accuracy')\n",
    "bernoulli_nb_scores_prec = cross_val_score(bernoulli_nb, X_train, y_train, cv=10, scoring='precision')\n",
    "bernoulli_nb_scores_rec = cross_val_score(bernoulli_nb, X_train, y_train, cv=10, scoring='recall')\n",
    "bernoulli_nb_scores_f1 = cross_val_score(bernoulli_nb, X_train, y_train, cv=10, scoring='f1')\n",
    "\n",
    "# Perform 10-fold cross-validation and compute performance metrics for Multinomial Naive Bayes\n",
    "multinomial_nb_scores_acc = cross_val_score(multinomial_nb, X_train, y_train, cv=10, scoring='accuracy')\n",
    "multinomial_nb_scores_prec = cross_val_score(multinomial_nb, X_train, y_train, cv=10, scoring='precision')\n",
    "multinomial_nb_scores_rec = cross_val_score(multinomial_nb, X_train, y_train, cv=10, scoring='recall')\n",
    "multinomial_nb_scores_f1 = cross_val_score(multinomial_nb, X_train, y_train, cv=10, scoring='f1')\n",
    "\n",
    "# Perform 10-fold cross-validation and compute performance metrics for Gaussian Naive Bayes\n",
    "gaussian_nb_scores_acc = cross_val_score(gaussian_nb, X_train, y_train, cv=10, scoring='accuracy')\n",
    "gaussian_nb_scores_prec = cross_val_score(gaussian_nb, X_train, y_train, cv=10, scoring='precision')\n",
    "gaussian_nb_scores_rec = cross_val_score(gaussian_nb, X_train, y_train, cv=10, scoring='recall')\n",
    "gaussian_nb_scores_f1 = cross_val_score(gaussian_nb, X_train, y_train, cv=10, scoring='f1')\n",
    "\n",
    "# Print the results\n",
    "print(\"Bernoulli Naive Bayes:\")\n",
    "print(\"Accuracy: {:.3f}\".format(bernoulli_nb_scores_acc.mean()))\n",
    "print(\"Precision: {:.3f}\".format(bernoulli_nb_scores_prec.mean()))\n",
    "print(\"Recall: {:.3f}\".format(bernoulli_nb_scores_rec.mean()))\n",
    "print(\"F1 Score: {:.3f}\".format(bernoulli_nb_scores_f1.mean()))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Multinomial Naive Bayes:\")\n",
    "print(\"Accuracy: {:.3f}\".format(multinomial_nb_scores_acc.mean()))\n",
    "print(\"Precision: {:.3f}\".format(multinomial_nb_scores_prec.mean()))\n",
    "print(\"Recall: {:.3f}\".format(multinomial_nb_scores_rec.mean()))\n",
    "print(\"F1 Score: {:.3f}\".format(multinomial_nb_scores_f1.mean()))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Gaussian Naive Bayes:\")\n",
    "print(\"Accuracy: {:.3f}\".format(gaussian_nb_scores_acc.mean()))\n",
    "print(\"Precision: {:.3f}\".format(gaussian_nb_scores_prec.mean()))\n",
    "print(\"Recall: {:.3f}\".format(gaussian_nb_scores_rec.mean()))\n",
    "print(\"F1 Score: {:.3f}\".format(gaussian_nb_scores_f1.mean()))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e4647a",
   "metadata": {},
   "source": [
    "Discussion:-\n",
    "\n",
    "Based on the above results, Bernoulli Naive Bayes performed the best among the three variants of Naive Bayes classifiers in terms of accuracy, precision, recall, and F1 score. It achieved the highest accuracy of 0.887 and the highest F1 score of 0.847.\n",
    "It can be due to the target variable has Binary outcomes.\n",
    "\n",
    "One limitation of Naive Bayes classifiers, including Bernoulli Naive Bayes and Multinomial Naive Bayes, is their assumption of independence among features. They assume that all features are conditionally independent given the class label. So this can be futher analysed.\n",
    "\n",
    "Another limitation of Bernoulli Naive Bayes and Multinomial Naive Bayes is their inability to handle continuous or real-valued features directly. They are designed for discrete features, such as binary or count data. In contrast, Gaussian Naive Bayes is specifically designed for continuous features, but it assumes that the feature distributions are Gaussian, which may not always be true in practice.\n",
    "\n",
    "Additionally, Naive Bayes classifiers are generally considered to be relatively simple and may not always capture complex patterns or interactions among features effectively, especially in datasets with high-dimensional or complex feature spaces. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
